# PySpark Merchant API Data Pipeline

## ğŸ“Œ Project Overview
This project demonstrates a real-world **Data Engineering pipeline** using **PySpark** and **REST APIs**.
Merchant data is ingested from an external API, cleaned, enriched, and stored in Parquet format.

## ğŸ›  Tech Stack
- Python
- PySpark
- REST API
- Google Colab
- GitHub

## ğŸ— Architecture
API â†’ PySpark â†’ Cleaning & Transformation â†’ Parquet Output

## ğŸ“¥ Data Source
Public REST API:
https://dummyjson.com/users

## ğŸ”„ Data Processing Steps
1. Fetch data from REST API
2. Flatten nested JSON structures
3. Apply explicit schema
4. Clean data (null handling, duplicates)
5. Enhance data (derived columns)
6. Write optimized Parquet output

## â–¶ How to Run
```bash
pip install pyspark requests
